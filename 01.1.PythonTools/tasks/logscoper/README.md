# LogScoper

![img.png](img.png)

Домашка немного уходит в инфраструктуру.
Мы в этой домашке попробуем с вами написать CLI (так в народе называются консольные утилиты) -- инструмент, которые читает входящие логи.

Что такое лог?

Лог - это текст, который отправляет система. Систем в системе может быть достаточно много. Из-за этого логи помечаются с какой системы они идут. И их всегда достаточно много.

Что мы сделаем в этой утилите:
* научим ее читать access-логи (поддержим формат NGINX)
* научим ее фильтровать по диапазону времени
* научим ее фильтровать по статус-кодам и по регулярным выражениям
* научим ее считать агрегаты: распределение по HTTP запросам, квантили 95ые и 99ые тайминги ответов
* научим ее строить гистрограммы времени ответа по МС "боксам"

Так, так. Что такое NGINX?
(кстати тут повнимательнее, такие вопросы часто спрашивают на интервью!!!)
Предположим, вы видите сайт. Пользователь открывает браузер и вводит запрос `aboba.net`. Чтобы страничка открылась и вы увидели абобу, перед сервером есть такая **программа**, которая как-то управляет запросами которые ей приходят.

Одна из таких -- NGINX. Его почти всегда используют и почему:
* очень быстро отдаёт картинки и все то что нужно сайту
* может распределять нагрузку между виртуалками. Ага! Сайт то может крутиться на множество виртуальных машин! Таких же но с большой или меньшей конфигурацией (параметрами с размером диска, процессора, оперативы и тд), которые вы получили на курсе АКОСа (или только получите)
* умеет работать с https (более подробнее на этом курсе мы говорить про него не будем, вспомним только тогда, когда вы если захотите продеплоить (сделать его видимым миру) свой сайт на итоговом проекте)
* и самое важное что умеет он делать: отправлять все внешние запросы прямо на сервер, на основное приложение которое вы подняли и где живет ваш сайт

(функций кастомного nginx могут быть и больше -- и они все конфигурируются и некоторые компании переписывают их под себя -- уточняйте, мы говорим про самый обычный nginx)

А так, какой пулл его работы?
Каждый запрос, который проходит через Nginx, он аккуратно записывает в журнал — это называется **access-лог**. Это просто текстовый файл, в котором каждая строка = один запрос.

Пример строки (немного упрощённый):

```nginx
127.0.0.1 - [10/Oct/2000:13:55:36 +0000] "GET /index.html" 200 2326 0.120
```

1. 127.0.0.1 - адрес клиента
2. [10/Oct/2000:13:55:36 +0000] - дата и время и часовой пояс сервера
3. 200 - код ответа
4. 0.120 - сколько секунд заняла обработка запроса

## О задаче

### Что нужно реализовать

Утилита запускаться должна так:


```
python -m logscoper <команда> [опции]
```

1. Команда `stats` 
Что делает: считает статистику по файлу логов.

* `total` - сколько всего строк/запросов прочитано
* `status` - словарь с количеством запросов по каждому HTTP-коду (например: {200: 69 , 404: 96, 405: 3}).
* `rt_avg_ms` - среднее время ответа (в миллисекундах, округлять до 2 знаков). Если нет данных о времени ответа — вывести `n/a`.
* `rt_p95_ms` — 95-й перцентиль времени ответа
* `rt_p99_ms` - 99-й перцинтель времени ответа
* `top_paths` — список из N самых популярных путей по файлу логов (например: ["/index.html", "/login", "/api"]), вместе с количеством запросов.


**Опции:**
*`--path access.log` — путь к файлу лога.  
*`--top 5` — сколько самых популярных URL выводить (по умолчанию 10).  
* `--since 2025-09-10T12:00:00+00:00` — учитывать только записи после этой даты/времени.  
* `--until 2025-09-10T13:00:00+00:00` — учитывать только записи до этого момента.  
* `--status 5xx` или `--status 200,404` — фильтр по статусам.  
* `--grep "regex"` — фильтр по пути (например, `--grep "^/api"`).  
*`--json` — вывод в формате JSON (иначе — текстом).

**Примеры:**

```bash
python -m logscoper stats --path access.log --top 3
```

Ответ:
Обычный текст если не указали флаг.

```
Total: 5
By status:
  200: 3
  302: 1
  500: 1
Avg RT (ms): 334.00
P95 RT (ms): 900.00
Top paths:
      1  /index.html
      1  /style.css
      1  /login
```

С флагом:

```bash
python -m logscoper stats --path access.log --top 3 --json
```


```
{
  "total": 5,
  "status": {"200": 3, "302": 1, "500": 1},
  "rt_avg_ms": 334.0,
  "rt_p95_ms": 900.0,
  "top_paths": [["/index.html", 1], ["/style.css", 1], ["/login", 1]]
}
```


**Пример 2**.

**Пример входных данных (`access.log`):**

```nginx
127.0.0.1 - - [10/Oct/2000:13:55:36 +0000] "GET /index.html HTTP/1.1" 200 2326 "-" "UA" 0.120
127.0.0.1 - - [10/Oct/2000:13:55:37 +0000] "GET /login HTTP/1.1" 302 0 "-" "UA" 0.250
127.0.0.1 - - [10/Oct/2000:13:55:38 +0000] "POST /api/v1/users HTTP/1.1" 500 12 "-" "UA" 0.900
127.0.0.1 - - [10/Oct/2000:13:55:40 +0000] "GET /index.html HTTP/1.1" 200 2326 "-" "UA" 0.110
```
Запрос:
```
python -m logscoper stats --path access.log --top 2
```
Ответ:
```
Total: 4
By status:
  200: 2
  302: 1
  500: 1
Avg RT (ms): 345.00
P95 RT (ms): 900.00
P99 RT (ms): 900.00
Top paths:
      2  /index.html
      1  /login

```

C `--json`:

Запрос:
```
python -m logscoper stats --path access.log --json --top 2
```
Ответ:

```json
{
  "total": 4,
  "status": {"200": 2, "302": 1, "500": 1},
  "rt_avg_ms": 345.0,
  "rt_p95_ms": 900.0,
  "rt_p99_ms": 900.0,
  "top_paths": [["/index.html", 2], ["/login", 1]]
}
```

2. Команда `filter`

Что делает: выводит только те строки, которые подходят под условия фильтрации.

Формат вывода: каждая подходящая запись печатается в нормализованном виде одной строкой:

**Опции:**
- `--path access.log` — путь к файлу лога.
- `--since 2025-09-10T12:00:00+00:00` — учитывать записи **начиная с** этой даты/времени (включительно).
- `--until 2025-09-10T13:00:00+00:00` — учитывать записи **до** этого момента (не включая).
- `--status 5xx` или `--status 200,404` — фильтр по статусам.
- `--grep "regex"` — фильтр по пути (регулярное выражение).
- `--out filtered.txt` — если задано, писать вывод в файл; иначе — в stdout.

**Примеры запуска:**

**Пример 1**


**Пример входных данных (`access.log`):**

```
127.0.0.1 - - [10/Oct/2000:13:55:36 +0000] "GET /index.html HTTP/1.1" 200 2326 "-" "UA" 0.120
127.0.0.1 - - [10/Oct/2000:13:55:40 +0000] "GET /boom HTTP/1.1" 500 0 "-" "UA" 0.900
```
Запрос:

```bash
python -m logscoper filter --path access.log --status 5xx
```

Ответ:

```
2000-10-10T13:55:38+00:00 127.0.0.1 POST /api/v1/users 500 12 rt=0.9
```

C `--grep`:
Запрос:
```
python -m logscope filter --path access.log --grep "^/index"
```

Ответ:
```
2000`-10-10T13:55:36+00:00 127.0.0.1 GET /index.html 200 2326 rt=0.12
```

C `--since`:

Запрос:
```shell
python -m logscoper filter --path access.log --since 2000-10-10T13:55:37+00:00
```

Ответ:

```text
2000-10-10T13:55:37+00:00 127.0.0.1 GET /login 302 0 rt=0.25
2000-10-10T13:55:38+00:00 127.0.0.1 POST /api/v1/users 500 12 rt=0.9
2000-10-10T13:55:40+00:00 127.0.0.1 GET /index.html 200 2326 rt=0.11
```

Фильтр по регулярке:

Запрос:

```shell
python -m logscoper filter --path access.log --grep "^/api"
```

Ответ:

```
2000-10-10T13:55:38+00:00 127.0.0.1 POST /api/v1/users 500 12 rt=0.9
```


3. Команда `hist`

Что делает: строит гистограмму по времени ответа (request_time). Каждый запрос попадает в “корзину” шириной bucket-ms миллисекунд.

- `--path access.log` — путь к файлу лога.

- `--bucket-ms 200` — ширина корзины (в мс). По умолчанию 100.

- `--since, --until, --status, --grep` — те же фильтры, что и для stats/filter.

- `--json` — вывод в формате JSON (иначе — текстом).

- `--strict` — если в логах нет request_time, вернуть ненулевой код выхода.


Пример того, что идет на вход.

```
127.0.0.1 - - [10/Oct/2000:13:55:36 +0000] "GET /index.html HTTP/1.1" 200 2326 "-" "UA" 0.120
127.0.0.1 - - [10/Oct/2000:13:55:37 +0000] "GET /style.css HTTP/1.1" 200 123 "-" "UA" 0.050
127.0.0.1 - - [10/Oct/2000:13:55:40 +0000] "GET /boom HTTP/1.1" 500 0 "-" "UA" 0.900
```

Пример запуска...


```
python -m logscope hist --path access.log --bucket-ms 200
```

Ответ:

```text
      0-200: ## 2
    200-400: # 1
    800-1000: # 1
```

С `--json`:

Запрос:

```shell
python -m logscoper hist --path access.log --bucket-ms 200 --json
```

Ответ:

```json
{
  "0-200": 2,
  "200-400": 1,
  "800-1000": 1
}
```



### Что за вас уже сделано и как начать что-то делать...
1. За вас уже написали интерфейс и разработали минимальную структуру папок (которую можно менять, но в рамках того, что тесты пройдут
2. Открывайте папку с тестами -- попробуйте запустить `tests/test_cli.py` -- если вы правильно поставили зависимости, то должно все заработать!
3. Далее, откройте любой тест и попробуйте его запустить -- если не работает, значит не работает. Вы еще ничего не сделали, он не должен работать :-)
4. Советую начать с написания модуля stat. Откройте файл `tests/test_stat.py`, посмотрите аргумент мока-лога из первого теста, попробуйте прыгнуть в него и посмотреть что там за такой лог. Позже посмотрите, что ожидается в `assert`'ах этого теста, и попробуйте придумать определенное решение
5. Если хотите собрать утилиту и поиграться локально на данных которые мы вам предоставим для теста то сделать можно так:

y



### Требования по обработке ошибок и деталям формата

* За хардкод тестов максимальный балл -- 0.5. Больше не будет

* Если файл не существует — печатать понятную ошибку и возвращать ненулевой код выхода.

* Времена --since/--until задаются в ISO-8601. Если часовой пояс не указан, считать UTC.

Для stats:

* rt_avg_ms и перцентили считаются только по записям, где есть request_time.

* Если request_time отсутствует везде, в текстовом выводе показать n/a; в JSON — null.

Для filter:

* Если bytes_sent равно - в исходном логе, выводить None или - (на выбор курса; главное — единообразно). Рекомендуем -.

* rt= печатать с исходной точностью (например 0.120 → 0.12 допустимо, но лучше не обрезать значащие нули).

Для hist:

* Корзина [k, k+bucket) (левая граница включительно, правая — нет), кроме последней, которая может быть “хвостовой”.

* Ключи корзин в JSON — строки вида "0-200", "200-400" и т. д.



## Любимая рубрика -- хинты

### Регулярки

Мы уже оставили для вас заготовки в `cli.py` — там определены две регулярки:

```python
import re
LOG_RE = re.compile(
    r'(?P<ip>\S+)\s+\S+\s+\S+\s+\[(?P<ts>[^\]]+)\]\s+'
    r'"(?P<method>[A-Z]+)\s+(?P<path>.*?)(?:\s+HTTP/\d\.\d)?"\s+'
    r'(?P<status>\d{3})\s+(?P<bytes>\S+)'
    r'(?:\s+"[^"]*"\s+"[^"]*")?'
    r'(?:\s+(?P<rt>\d+\.\d+)|\s+rt=(?P<rt_kv>\d+\.\d+))?'
)

RT_KV_RE = re.compile(r'(?:^|\s)rt=(?P<rt>\d+\.\d+)\b')
```

Что нужно сделать вам: 
* разобраться с ними, что они делают
* написать новый модуль, которая принимает строку лога и возвращает объект LogEntry
* Поля из регулярок должны быть распарсены и приведены к нужным типам
* если строка не парсится -- возвращайте None

### Как считать 99 и 95ые перцентели

Перцентиль — это такая "отсечка", которая показывает, какое значение находится на границе X% всех данных.


**Пример**: у нас есть 100 измерений времени ответа.

* 95-й перцентиль (p95) — это значение, ниже которого лежат 95% всех запросов.  
* 99-й перцентиль (p99) — это значение, ниже которого лежат 99% всех запросов.  


В продакшене часто смотрят именно на p95 и p99, потому что среднее время (avg) может обманывать. Например:  
* 99 запросов по 100 мс и 1 запрос на 10 секунд.  
* Среднее = ~200 мс, а p95 = 100 мс. Это честнее отражает типичное поведение.

### Как запустить ровно также как и в примерах?

Тут мы продублируем, на семинаре мы рассмотрели варианты сборки утилит, тут нужно сделать также.

Для начала установите библиотеку `pyinstaller`

```shell
pip install pyinstaller
```

Cоберите ее:

```shell
<ваш питон> -m pyinstaller --onefile -n logscoper -m logscoper.cli
```

После сборки бинарник окажется у вас в папке `./dist/`

Что нужно сделать дальше? Запустить!

```
logscoper stats --path <путь к вашим логам>
```